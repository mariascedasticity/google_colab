# -*- coding: utf-8 -*-
"""report_data_cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19SA4lU-afR9LjvahHrlQs4V44dvRIIjS
"""

# @title Setup
from google.colab import auth
from google.cloud import bigquery
from google.colab import data_table
import pandas as pd
from datetime import datetime
import numpy as np


pd.set_option('display.max_columns', 50)  # Set the maximum number of columns to 50

# GCP settings
project = 'willow-health' # Project ID inserted based on the query results selected to explore
location = 'US' # Location inserted based on the query results selected to explore
client = bigquery.Client(project=project, location=location)
data_table.enable_dataframe_formatter()
auth.authenticate_user()

# Running this code will display the query used to generate your previous job

# Run the following code to get the latest JOB ID from Cloud Shell Terminal:
# bq ls -j -a --max_results=1 --format=prettyjson | jq '.[0].jobReference.jobId'

LOAD_LOCAL = False # CHANGE THIS TO FALSE TO RUN FROM CLOUD

if LOAD_LOCAL:
  results = pd.read_csv("/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/RealTime_sleep_survey_stand_raw_latest_05_07_2023.csv")
  results_hr = pd.read_csv("/content/drive/MyDrive/AI and Mental Health Project/MIT_Pilot_Analysis/New_Sample_Users_RealTime_hours_raw_latest_05_07_2023.csv")
else:
  JOB_ID = 'bquxjob_14f7c316_188c9b696e2' # Sleep
  JOB_HR_ID = 'bquxjob_6ccf20b7_188c9b6a1ca'
  JOB_SURVEY_ID = 'bquxjob_32eb413b_188c9b66f36'

  # Running this code will read results from your previous job

  job_realtime_sleep_survey = client.get_job(JOB_ID) # Job ID inserted based on the query results selected to explore
  print(job_realtime_sleep_survey.query)
  results = job_realtime_sleep_survey.to_dataframe()

  job_realtime_hr = client.get_job(JOB_HR_ID)
  print(job_realtime_hr.query)
  results_hr = job_realtime_hr.to_dataframe()

  job_realtime_survey = client.get_job(JOB_SURVEY_ID)
  print(job_realtime_survey.query)
  results_survey = job_realtime_survey.to_dataframe()

results_hr

# @title Helper Functions

def modify_sleep_dataset(dataset):
    # Change userid column values
    dataset['userid'] = dataset['userid'].str.strip('"')

    # Change column name document_id to date and convert to date format
    dataset.rename(columns={'document_id': 'date'}, inplace=True)
    dataset['date'] = pd.to_datetime(dataset['date']).dt.strftime('%Y-%m-%d')

    # Rename start_time and end_time columns
    dataset.rename(columns={'start_time': 'start_time_sleep', 'end_time': 'end_time_sleep'}, inplace=True)

    return dataset

def convert_to_24h(time_str):
    if 'am' in time_str or 'pm' in time_str:
        return datetime.strptime(time_str, '%I:%M:%S %p').strftime('%H:%M:%S')
    else:
        return time_str

def modify_hr_dataset(dataset):
    # replace "None" string with numpy NaN (which is the proper representation for missing values)
    dataset.replace("None", np.nan, inplace=True)

    dataset['hours'] = dataset['hours'].apply(convert_to_24h)
    dataset = dataset.dropna(axis=1, how='all')

    # convert all columns from the 5th onwards (Python is 0-indexed, so column at index 4 is the 5th column)
    # note: this includes the 5th column
    for col in dataset.columns[4:]:
      dataset[col] = pd.to_numeric(dataset[col], errors='coerce')
      dataset = dataset.sort_values(by=['userid', 'date'])


    variables_to_keep = [
        'userid',
        'hours',
        'timestamp',
        'date',
        'heart_rate_avg',
        'heart_rate_min',
        'heart_rate_max',
        'heart_rate_variability_sdnn_avg',
        'active_energy_burned',
        'basal_energy_burned',
        'step_count',
        'distance_cycling_nested',
        'flights_climbed',
        'apple_exercise_time',
        'oxygen_saturation_avg'
    ]

    dataset = dataset.filter(variables_to_keep)

    return dataset

def modify_survey_dataset(dataset):
    dataset.rename(columns={'document_id': 'date'}, inplace=True)
    dataset['date'] = pd.to_datetime(dataset['date']).dt.strftime('%Y-%m-%d')
    dataset = dataset.sort_values(by=['userid', 'date'])

    return dataset

results = modify_sleep_dataset(results)
results_survey = modify_survey_dataset(results_survey)
results_hr = modify_hr_dataset(results_hr)

# OFNaNat6f6gMrgL1SS5MGzHWjrB2

filtered_hr = results_hr[results_hr['userid'] == 'OFNaNat6f6gMrgL1SS5MGzHWjrB2']
filtered_survey = results_survey[results_survey['userid'] == 'OFNaNat6f6gMrgL1SS5MGzHWjrB2']
filtered_sleep = results[results['userid'] == 'OFNaNat6f6gMrgL1SS5MGzHWjrB2']

pd.set_option('display.max_columns', 50)  # Set the maximum number of columns to 50

filtered_hr.describe()

filtered_hr = filtered_hr.sort_values(by=['date', 'hours'])

# Here I create different metrics to aggregate in daily data

aggregated_df = filtered_hr.groupby(['userid', 'date']).agg({
    'heart_rate_avg': ['mean', 'min', 'max'],
    'heart_rate_min': ['mean', 'min', 'max'],
    'heart_rate_max': ['mean', 'min', 'max'],
    'heart_rate_variability_sdnn_avg': ['mean', 'min', 'max'],
    'active_energy_burned': 'sum',
    'basal_energy_burned': 'sum',
    'step_count': 'sum',
    'distance_cycling_nested': 'sum',
    'flights_climbed': 'sum',
    'apple_exercise_time': 'sum',
    'oxygen_saturation_avg': ['mean', 'min', 'max'],
}).reset_index()

aggregated_df.columns = ['_'.join(col).strip() for col in aggregated_df.columns.values]

# If any of the aggregate is equal to zero, take an average of the past week
for column in aggregated_df.columns:
    # Skip the 'userid_' and 'date_' columns
    if column not in ['userid_', 'date_']:
        # Loop over each user
        for user in aggregated_df['userid_'].unique():
            # Create a mask for the current user
            mask = aggregated_df['userid_'] == user
            # Replace zeros with NaN
            aggregated_df.loc[mask, column] = aggregated_df.loc[mask, column].replace(0, np.nan)
            # Fill NaN values with the mean of the past 7 days
            aggregated_df.loc[mask, column] = aggregated_df.loc[mask, column].fillna(aggregated_df[mask][column].rolling(7, min_periods=1).mean())

# Replace 'userid_' and 'date_' with 'userid' and 'date'
aggregated_df.rename(columns={'userid_': 'userid', 'date_': 'date'}, inplace=True)
aggregated_df

merged_df = pd.merge(filtered_survey, aggregated_df, on=['userid', 'date'], how='right')
merged_df

df = merged_df
df['date'] = pd.to_datetime(df['date'])

df['day_of_week'] = df['date'].dt.dayofweek
wellbeing_indicators = ['Today_I_felt_calm', 'How_stressed_out_are_you_today', 'Today_I_felt_nervous', 'What_time_of_the_day_were_you_the_most_stressed', 'Today_I_was_worried']
df = df.dropna(subset=wellbeing_indicators)
df

import matplotlib.pyplot as plt
import seaborn as sns

# Define the columns of interest
wellbeing_cols = ['Today_I_felt_calm', 'How_stressed_out_are_you_today', 'Today_I_felt_nervous', 'Today_I_was_worried']
activity_cols = ['heart_rate_avg_mean', 'heart_rate_avg_min', 'heart_rate_avg_max',
                 'heart_rate_min_mean', 'heart_rate_min_min', 'heart_rate_min_max',
                 'heart_rate_max_mean', 'heart_rate_max_min', 'heart_rate_max_max',
                 'heart_rate_variability_sdnn_avg_mean',
                 'heart_rate_variability_sdnn_avg_min',
                 'heart_rate_variability_sdnn_avg_max',
                 'active_energy_burned_sum', 'basal_energy_burned_sum',
                 'step_count_sum', 'distance_cycling_nested_sum',
                 'flights_climbed_sum', 'apple_exercise_time_sum',
                 'oxygen_saturation_avg_mean', 'oxygen_saturation_avg_min',
                 'oxygen_saturation_avg_max']

# Check if the columns exist in the DataFrame
wellbeing_cols = [col for col in wellbeing_cols if col in df.columns]
activity_cols = [col for col in activity_cols if col in df.columns]

# Subset the dataframe to only include columns of interest
df_subset = df[wellbeing_cols + activity_cols]

# Compute correlations
corr = df_subset.corr()

# Extract specific correlations
corr_specific = corr.loc[activity_cols, wellbeing_cols]

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

# Plot the specific correlations
f, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(corr_specific, cmap=cmap, annot=True, fmt=".2f", linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)
plt.show()

# @title Sleep

sleep_types_to_keep = ['in_bed', 'awake']
filtered_df = filtered_sleep[filtered_sleep['sleep_type'].isin(sleep_types_to_keep)]

filtered_df['start_time_sleep'] = pd.to_datetime(filtered_df['start_time_sleep'])
filtered_df['end_time_sleep'] = pd.to_datetime(filtered_df['end_time_sleep'])
filtered_df['diff'] = filtered_df['end_time_sleep'] - filtered_df['start_time_sleep']
filtered_df['diff'] = filtered_df['diff'].dt.total_seconds() / 60

filtered_df = filtered_df.sort_values(by='start_time_sleep', ascending=True)

# @title Sleep Function 1
# Function to sum up 'diff' for each date and userid with the same 'sleep_type' and 'source_type'
def sum_diff(df):
    grouped_df = df.groupby(['date', 'userid', 'sleep_type', 'source_type']).agg({'diff': 'sum'}).reset_index()
    return grouped_df

# Function to compute the difference between 'in_bed' and 'awake' sleep_types
def compute_difference(df):
    # First, pivot the DataFrame so that the sleep_types are columns and the diff sums are the values
    pivot_df = df.pivot_table(values='diff', index=['date', 'userid', 'source_type'], columns='sleep_type', fill_value=0).reset_index()

    # Then, calculate the difference between 'in_bed' and 'awake'
    pivot_df['in_bed_minus_awake'] = pivot_df['in_bed'] - pivot_df['awake']

    return pivot_df

# Usage
grouped_df = sum_diff(filtered_df)
result_df = compute_difference(grouped_df)

# @title Sleep Function 2

def sum_diff(df):
    # Split df into two parts
    df_less_than_150 = df[df['diff'] < 150]
    df_greater_than_150 = df[df['diff'] >= 150]

    # Group and sum for df_less_than_150
    grouped_df_less_than_150 = df_less_than_150.groupby(['date', 'userid', 'sleep_type', 'source_type']).agg({'diff': 'sum'}).reset_index()

    # Concatenate both parts
    result_df = pd.concat([grouped_df_less_than_150, df_greater_than_150], ignore_index=True)

    return result_df

grouped_df = sum_diff(filtered_df)
result_df = compute_difference(grouped_df)
result_df['hours'] = result_df['in_bed_minus_awake']/60